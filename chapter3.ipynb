{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(\"http://en.wikipedia.org/wiki/Kevin_Bacon\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "for link in bsObj.findAll(\"a\"):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in bsObj.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\",\n",
    "                        href=re.compile(\"^(/wiki/)((?!:).)*$\")):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "random.seed(datetime.datetime.now())\n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen(\"http://en.wikipedia.org\"+articleUrl)\n",
    "    bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "    return bsObj.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\",\n",
    "                        href=re.compile(\"^(/wiki/)((?!:).)*$\"))\n",
    "links = getLinks(\"/wiki/Kevin_Bacon\")\n",
    "while len(links) > 0:\n",
    "    newArticle = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n",
    "    print(newArticle)\n",
    "    links = getLinks(newArticle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = set()\n",
    "def getLinks(pageUrl):\n",
    "    global pages\n",
    "    html = urlopen(\"http://en.wikipedia.org\"+pageUrl)\n",
    "    bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "    for link in bsObj.findAll(\"a\", href=re.compile(\"^(/wiki/)\")):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages:\n",
    "                # 새 페이지를 발견\n",
    "                newPage = link.attrs['href']\n",
    "                print(\"----------------\\n\"+newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)\n",
    "            \n",
    "getLinks(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = set()\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "def getIntLinks(bsObj, includeUrl):\n",
    "    internalLinks = []\n",
    "    for link in bsObj.findAll(\"a\", href=re.compile(\"^(/|.*\"+includeUrl+\")\")): # 正则表达式: 以/开头或者包含includeUrl\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in internalLinks:\n",
    "                if(link.attrs['href'].startswith(\"/\")):\n",
    "                    internalLinks.append(includeUrl+link.attrs['href'])\n",
    "                else:\n",
    "                    internalLinks.append(link.attrs['href'])\n",
    "    return internalLinks\n",
    "\n",
    "def getExternalLinks(bsObj, excludeUrl):\n",
    "    externalLinks = []\n",
    "    for link in bsObj.findAll(\"a\",\n",
    "                        href=re.compile(\"^(http|www)((?!\"+excludeUrl+\").)*$\")): # 正则表达式: 以http或www开头, 但不包含excludeUrl\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in externalLinks:\n",
    "                externalLinks.append(link.attrs['href'])\n",
    "    return externalLinks\n",
    "\n",
    "def splitAddress(address):\n",
    "    addressParts = address.replace(\"http://\", \"\").split(\"/\") \n",
    "    return addressParts\n",
    "\n",
    "def getRandomExternalLink(startingPage):\n",
    "    html = urlopen(startingPage)\n",
    "    bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "    externalLinks = getExternalLinks(bsObj, splitAddress(startingPage)[0]) # splitAddress(startingPage)[0] = domain\n",
    "    if len(externalLinks) == 0:\n",
    "        domain = splitAddress(startingPage)[0]\n",
    "        internalLinks = getIntLinks(bsObj, domain)\n",
    "        return getRandomExternalLink(internalLinks[random.randint(0,\n",
    "                                                len(internalLinks)-1)])\n",
    "    else:\n",
    "        return externalLinks[random.randint(0, len(externalLinks)-1)]\n",
    "    \n",
    "def followExternalOnly(startingSite):\n",
    "    externalLink = getRandomExternalLink(startingSite)\n",
    "    print(\"Random external link is: \"+externalLink)\n",
    "    followExternalOnly(externalLink)\n",
    "\n",
    "followExternalOnly(\"http://oreilly.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allExtLinks = set()\n",
    "allIntLinks = set()\n",
    "def getAllExternalLinks(siteUrl):\n",
    "    html = urlopen(siteUrl)\n",
    "    domain = splitAddress(siteUrl)[0]\n",
    "    bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "    internalLinks = getIntLinks(bsObj, domain)\n",
    "    externalLinks = getExternalLinks(bsObj, domain)\n",
    "    for link in externalLinks:\n",
    "        if link not in allExtLinks:\n",
    "            allExtLinks.add(link)\n",
    "            print(link)\n",
    "    for link in internalLinks:\n",
    "        if link not in allIntLinks:\n",
    "            print(\"About to get link: \"+link)\n",
    "            allIntLinks.add(link)\n",
    "            getAllExternalLinks(link)\n",
    "\n",
    "getAllExternalLinks(\"http://oreilly.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Item, Field,Selector,Spider\n",
    "class Article(Item):\n",
    "    title = Field()\n",
    "    text = Field()\n",
    "\n",
    "class ArticleSpider(Spider):\n",
    "    name = \"article\"\n",
    "    allowed_domains = [\"en.wikipedia.org\"]\n",
    "    start_urls = [\"http://en.wikipedia.org/wiki/Main_Page\",\n",
    "                  \"http://en.wikipedia.org/wiki/Python_%28programming_language%29\"]\n",
    "    def parse(self, response):\n",
    "        item = Article()\n",
    "        title = response.xpath('//h1/text()')[0].extract()\n",
    "        print(\"Title is: \"+title)\n",
    "        item['title'] = title\n",
    "        return item\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
